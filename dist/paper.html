<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <script defer src="https://use.fontawesome.com/releases/v5.2.0/js/all.js" integrity="sha384-4oV5EgaV02iISL2ban6c/RmotsABqE4yZxZLcYMAdG7FAPsyHYAPpywE9PJo+Khy" crossorigin="anonymous"></script>
  <link href="https://fonts.googleapis.com/css?family=Lato|Open+Sans|Roboto|Source+Sans+Pro" rel="stylesheet">
  <link rel="stylesheet" href="css/main.css">

  <title>Welcome to My Portofolio</title>
</head>
<body id="bg-img">

    <header>
        <div class="menu-btn">
          <div class="btn-line"></div>
          <div class="btn-line"></div>
          <div class="btn-line"></div>
    
        </div>
        <nav class="menu">
          <div class="menu-branding">
            <div class="portrait">
            </div>
          </div>
          <ul class="menu-nav">
            <li class="nav-item current">
              <a href="index.html" class="nav-link">Home</a>
            </li>
            <li class="nav-item"><a href="about.html" class="nav-link">About Me</a></li>
            <li class="nav-item"><a href="work.html" class="nav-link">My Work</a></li>
            <li class="nav-item"><a href="/" class="nav-link">How to Reach Me</a></li>
          </ul>
        </nav>
      </header>
      <script src="js/main.js"></script>
<main id="paper">
    <h1>The Applied Study of Over-parameterized Neural Networks</h1>
    <p>Check out <a href="img/Paper Johnson Zhou.pdf" class="link" download>my paper</a>! </p>
    <h2>Background and abstract</h2>
    <p>My paper is based on <a href="https://arxiv.org/abs/1803.01206" class="link">On the Power of Over-parametrization in Neural Networks with Quadratic Activation</a>. This paper by Du and Lee performed the theoretical deductions of the property of over-parameterized one-layer networks with quadratic activation to have all local minima as global minima. My paper performs an empirical analysis of their paper and extended the activation functions to ReLU. My conclusions suggests that 'over-parameterized definition' of their paper is weak as many other cases showcase the same property of global convergence. I explored the boundary for such property in one-layer network with quadratic and ReLU activation.</p>
    <h2>Sample images</h2>
    <div class="imgs">
      <div class="img"><img src="img/Sample 1.png" alt="1">
        <p>NN with Quadratic activation's loss (log)</p>
      </div>
      <div class="img"><img src="img/Sample 2.png" alt="2">
        <p class="p2">NN with ReLU activation's loss</p>
      </div>
    </div>
    <br>
    <h2>Evaluation by my Professor</h2>
    <p>Check out the <a href="img/Pioneer_Evaluation_Sheet_Johnson.pdf" class="link"> Evaluation</a></p>
</main>


</body>